{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 1. Navigation\n",
    "I trained agent to collect yellow bananas and avoid blue bananas using Deep-Q-Learning\n",
    "![alt text](/play.gif)\n",
    "## Environment\n",
    "yellow bananas and blue bananas randomly dropped in floor\n",
    "\n",
    "#### State\n",
    "The state space has 37 dimensions and contains the agent's velocity, along with ray-based perception of objects around agent's forward direction\n",
    "\n",
    "#### Action\n",
    "* 0 - move forward.\n",
    "* 1 - move backward.\n",
    "* 2 - turn left.\n",
    "* 3 - turn right.\n",
    "\n",
    "#### Reward\n",
    "A reward of +1 is provided for collecting a yellow banana, and a reward of -1 is provided for collecting a blue banana.\n",
    "\n",
    "#### Goal\n",
    "agent must get an average score of +13 over 100 consecutive episodes.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Algorithms\n",
    "I read paper and programming it myself. but, I'm poor at writing in English. report's algorithm explanation a little bit referenced by other student's report.\n",
    "\n",
    "#### DQN\n",
    "DQN evolved from Q-learning using function approximator powered by deep neural network. \n",
    "it uses fixed Q-targets and experience replay for stabilizing learning.\n",
    "\n",
    "#### Fixed Q-Targets\n",
    "In Q-Learning, algorithm updates a guess with a guess, and this can potentially lead to harmful correlations. To avoid this, algorithm uses two separate networks with identical architectures(local Q-Network and target Q-Network). after that, the target Q-Network update less often than the local Q-Network. it's decoupling the target network from local Q-Network. it's giving it a more stable learning environment.\n",
    "\n",
    "#### Experience Replay\n",
    "When the agent interacts with the environment, the sequence of experience tuples can be highly correlated. To break harmful correlations, algorithm contains a collection of experience tuples and samples a small batch of tuples in order to learn. it's experience replay.\n",
    "\n",
    "#### Double DQN\n",
    "DQN can makes a mistake in the early stages. because, the Q-values are still evolving, and algorithm may not have gathered enough information to figure out the best action. So, maybe algorithm shouldn't blindly trust DQN's selection. Double DQN decouples selection from evaluation. algorithm selects the best action using one set of parameters w, but evaluates it using a different set of parameters w'. this simple modification keeps Q-values, preventing them from exploding in early stages of learning or fluctuating later on.\n",
    "\n",
    "#### Prioritized Experience Replay\n",
    "Prioritized Experience Replay evolved from Experience Replay using priority. algorithm assigns priorities to each tuple from TD-error delta. when creating batches, algorithm uses this value to compute a sampling probability. when a tuple is picked, update its priority with a newly computed TD error using the latest q values.\n",
    "\n",
    "## Plot of Rewards\n",
    "![alt text](/Rewards.png)\n",
    "\n",
    "## Ideas for Future Work\n",
    "* RAINBOW : i want to challenge Rainbow later. it includes Dueling DQN, Distributional DQN, Noisy DQN and so on..\n",
    "* Learning from Pixels : i want to challenge Learning from Pixels later. i guess, it need studying CNN for solving problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
